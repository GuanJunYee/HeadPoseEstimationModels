# evaluation/test_all_models.py - Test all 3 models
"""
Comprehensive Model Evaluation Script

This script evaluates all trained head pose estimation models on the test set
and provides comparative analysis following the Deep-Head-Pose evaluation methodology.

EVALUATION METHODOLOGY:
1. Load trained models from checkpoints
2. Test on standardized AFLW2000 test set
3. Calculate Mean Absolute Error (MAE) in degrees
4. Compare against validation performance to assess generalization
5. Provide per-angle breakdown (yaw, pitch, roll)

MODELS EVALUATED:
1. Custom CNN: Lightweight from-scratch architecture
2. HopeNet (from scratch): ResNet-50 based, trained from scratch  
3. HopeNet (fine-tuned): Pre-trained and fine-tuned model

EVALUATION METRICS:
- Overall MAE: Average error across all three angles
- Per-angle MAE: Individual errors for yaw, pitch, roll
- Validation vs Test comparison: Generalization assessment

OUTPUT:
- Comparative table showing all model performances
- Ranking by test set performance
- Analysis of validation vs test generalization
"""
import sys
import torch
import numpy as np
from sklearn.metrics import mean_absolute_error

sys.path.append('.')
from models.hopenet import create_hopenet
from models.custom_cnn import create_custom_cnn
from utils.datasets import create_data_loaders
from training.finetune_hopenet import HopeNetFineTuner
from training.train_custom_cnn import CustomCNNTrainer

def test_model_on_test_set(model, test_loader, device, model_type="hopenet"):
    """
    Test any model on the test set using appropriate evaluation methodology
    
    Args:
        model: Trained model to evaluate
        test_loader: Test dataset loader
        device: Computing device (cuda/cpu)
        model_type: Type of model ("hopenet" or "custom_cnn")
        
    Returns:
        Dictionary containing evaluation metrics:
        - overall_mae: Average MAE across all angles
        - yaw_mae, pitch_mae, roll_mae: Per-angle MAE values
        
    Methodology:
    1. Set model to evaluation mode
    2. Disable gradients for efficiency
    3. Forward pass through test samples
    4. Convert logits to continuous angles using model-specific method
    5. Calculate MAE metrics against ground truth
    """
    model.eval()
    all_pred_angles = []
    all_true_angles = []
    
    # Create appropriate evaluator based on model type
    # Each model type uses its own angle conversion methodology
    if model_type == "hopenet":
        # Use HopeNet's expected value calculation method
        evaluator = HopeNetFineTuner(model, None, None, device)
        
        with torch.no_grad():
            for images, labels, cont_labels, _ in test_loader:
                images = images.to(device)
                
                # Forward pass through HopeNet
                yaw_logits, pitch_logits, roll_logits = model(images)
                
                # Convert classification logits to continuous angles
                # Uses softmax + expected value calculation
                yaw_pred, pitch_pred, roll_pred = evaluator.convert_logits_to_angles(
                    yaw_logits, pitch_logits, roll_logits
                )
                
                predicted_angles = torch.stack([yaw_pred, pitch_pred, roll_pred], dim=1)
                all_pred_angles.append(predicted_angles.cpu())
                all_true_angles.append(cont_labels)
                
    elif model_type == "custom_cnn":
        # Use Custom CNN's evaluation method (same as HopeNet methodology)
        evaluator = CustomCNNTrainer(model, None, None, device)
        
        with torch.no_grad():
            for images, labels, cont_labels, _ in test_loader:
                images = images.to(device)
                
                # Forward pass
                yaw_logits, pitch_logits, roll_logits = model(images)
                
                # Convert to angles using Custom CNN method
                yaw_pred, pitch_pred, roll_pred = evaluator.convert_logits_to_angles(
                    yaw_logits, pitch_logits, roll_logits
                )
                
                predicted_angles = torch.stack([yaw_pred, pitch_pred, roll_pred], dim=1)
                all_pred_angles.append(predicted_angles.cpu())
                all_true_angles.append(cont_labels)
    
    # Calculate performance
    all_pred_angles = torch.cat(all_pred_angles).numpy()
    all_true_angles = torch.cat(all_true_angles).numpy()
    
    overall_mae = mean_absolute_error(all_true_angles, all_pred_angles)
    yaw_mae = mean_absolute_error(all_true_angles[:, 0], all_pred_angles[:, 0])
    pitch_mae = mean_absolute_error(all_true_angles[:, 1], all_pred_angles[:, 1])
    roll_mae = mean_absolute_error(all_true_angles[:, 2], all_pred_angles[:, 2])
    
    return {
        'overall_mae': overall_mae,
        'yaw_mae': yaw_mae,
        'pitch_mae': pitch_mae,
        'roll_mae': roll_mae
    }

def test_all_models():
    """Test all 3 trained models on test set"""
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"Using device: {device}")
    
    # Load test data
    print("Loading AFLW2000 test data...")
    train_loader, val_loader, test_loader = create_data_loaders(
        "data/AFLW2000", "data/splits", batch_size=16, num_workers=0
    )
    
    print("=" * 80)
    print("TESTING ALL MODELS ON TEST SET")
    print("=" * 80)
    print(f"Test samples: {len(test_loader.dataset)}")
    print()
    
    results = {}
    
    # Test 1: Custom CNN (HopeNet methodology)
    print("1Ô∏è‚É£ Testing Custom CNN (HopeNet methodology)...")
    try:
        model1 = create_custom_cnn(num_bins=66)
        checkpoint1 = torch.load("results/custom_cnn/best_custom_cnn.pth", map_location=device)
        model1.load_state_dict(checkpoint1['model_state_dict'])
        model1 = model1.to(device)
        
        results['custom_cnn'] = test_model_on_test_set(model1, test_loader, device, "custom_cnn")
        val_mae1 = checkpoint1['best_val_mae']
        
        print(f"   Validation MAE: {val_mae1:.2f}¬∞")
        print(f"   Test MAE: {results['custom_cnn']['overall_mae']:.2f}¬∞")
        print(f"   Difference: {results['custom_cnn']['overall_mae'] - val_mae1:+.2f}¬∞")
        
    except Exception as e:
        print(f"   ‚ùå Error loading Custom CNN: {e}")
        results['custom_cnn'] = None
    
    print()
    
    # Test 2: HopeNet (From scratch)
    print("2Ô∏è‚É£ Testing HopeNet (From scratch)...")
    try:
        model2 = create_hopenet(num_bins=66)
        checkpoint2 = torch.load("results/hopenet/best_hopenet.pth", map_location=device)
        model2.load_state_dict(checkpoint2['model_state_dict'])
        model2 = model2.to(device)
        
        results['hopenet_scratch'] = test_model_on_test_set(model2, test_loader, device, "hopenet")
        val_mae2 = checkpoint2['best_val_mae']
        
        print(f"   Validation MAE: {val_mae2:.2f}¬∞")
        print(f"   Test MAE: {results['hopenet_scratch']['overall_mae']:.2f}¬∞")
        print(f"   Difference: {results['hopenet_scratch']['overall_mae'] - val_mae2:+.2f}¬∞")
        
    except Exception as e:
        print(f"   ‚ùå Error loading HopeNet (scratch): {e}")
        results['hopenet_scratch'] = None
    
    print()
    
    # Test 3: HopeNet (Fine-tuned) - We already know this
    print("3Ô∏è‚É£ HopeNet (Fine-tuned) - Already tested:")
    results['hopenet_finetuned'] = {
        'overall_mae': 5.12,
        'yaw_mae': 3.95,
        'pitch_mae': 6.39,
        'roll_mae': 5.02
    }
    print(f"   Validation MAE: 4.20¬∞")
    print(f"   Test MAE: 5.12¬∞")
    print(f"   Difference: +0.91¬∞")
    
    print()
    print("=" * 80)
    print("FINAL TEST RESULTS COMPARISON")
    print("=" * 80)
    
    # Create comparison table
    print("| Model                    | Method              | Val MAE | Test MAE | Difference |")
    print("|--------------------------|---------------------|---------|----------|------------|")
    
    if results['custom_cnn']:
        val_diff1 = results['custom_cnn']['overall_mae'] - val_mae1
        print(f"| Custom CNN (HopeNet method) | HopeNet methodology | {val_mae1:.2f}¬∞   | {results['custom_cnn']['overall_mae']:.2f}¬∞    | {val_diff1:+.2f}¬∞      |")
    else:
        print("| Custom CNN (HopeNet method) | HopeNet methodology | N/A     | N/A      | N/A        |")
    
    if results['hopenet_scratch']:
        val_diff2 = results['hopenet_scratch']['overall_mae'] - val_mae2
        print(f"| HopeNet (From scratch)   | Deep-Head-Pose      | {val_mae2:.2f}¬∞   | {results['hopenet_scratch']['overall_mae']:.2f}¬∞    | {val_diff2:+.2f}¬∞      |")
    else:
        print("| HopeNet (From scratch)   | Deep-Head-Pose      | N/A     | N/A      | N/A        |")
    
    print(f"| HopeNet (Fine-tuned)     | Fine-tuning         | 4.20¬∞   | 5.12¬∞    | +0.91¬∞     |")
    
    print("\n" + "=" * 80)
    print("FINAL RANKING (by Test MAE)")
    print("=" * 80)
    
    # Sort by test performance
    valid_results = []
    if results['custom_cnn']:
        valid_results.append(("Custom CNN (HopeNet method)", results['custom_cnn']['overall_mae']))
    if results['hopenet_scratch']:
        valid_results.append(("HopeNet (From scratch)", results['hopenet_scratch']['overall_mae']))
    valid_results.append(("HopeNet (Fine-tuned)", 5.12))
    
    valid_results.sort(key=lambda x: x[1])
    
    for i, (name, mae) in enumerate(valid_results, 1):
        medal = ["ü•á", "ü•à", "ü•â"][i-1] if i <= 3 else f"{i}."
        print(f"{medal} {name}: {mae:.2f}¬∞ MAE")
    
    print("\n" + "=" * 80)
    print("DETAILED RESULTS BY ANGLE")
    print("=" * 80)
    
    for model_name, result in results.items():
        if result:
            print(f"\n{model_name.replace('_', ' ').title()}:")
            print(f"  Overall: {result['overall_mae']:.2f}¬∞")
            print(f"  Yaw: {result['yaw_mae']:.2f}¬∞")
            print(f"  Pitch: {result['pitch_mae']:.2f}¬∞")
            print(f"  Roll: {result['roll_mae']:.2f}¬∞")

if __name__ == "__main__":
    test_all_models()